# HIS2LIP Leveraging Weighted Inter-and-Intramodal Soft Embeddings Contrastive Loss for IHC Image Analysis.

## üìå Overview  
Immunohistochemistry (IHC) is essential in cancer diagnosis, and treatment planning, but its analysis remains manual, time-consuming, and prone to variability. Existing AI models for histopathology either focus on single modalities or rely on vision-language models (VLMs) originally developed for H&E slides or radiology, which do not capture the unique dual-level nature of IHC (tissue type and biomarker).  

We present **HIS2LIP**, the first VLM specifically designed for IHC analysis. HIS2LIP introduces a **Weighted Inter- and Intramodal Soft Embeddings Contrastive Loss (WISE)** to:  
- Mitigate false negatives common in hard contrastive learning.  
- Align both **across modalities** (image-text) and **within modalities** (image-image, text-text).  
- Integrate domain-specific contextual information (e.g., tissue type and biomarker).  

HIS2LIP builds on three popular VLM backbones‚Äî**CLIP, PLIP, and BiomedCLIP**‚Äîresulting in HIS2CLIP, HIS2PLIP, and HIS2BiomedCLIP.  
The models are fine-tuned on **MIHIC**, the largest public IHC dataset for lung cancer, enriched with **7,000 expert-validated captions** generated by GPT-4 and Llama-3 in close collaboration with pathologists.  

**Key Results:**  
- Up to **3.5√ó improvement** in zero-shot classification over CLIP.  
- Up to **3.21√ó gains** in multimodal retrieval (image-to-text, text-to-text, image-to-image).  
- **Analysis time reduced** from ~15 minutes per patch to ~6 seconds while maintaining high accuracy.  

## ‚ö†Ô∏è Notes  
- Supplementary material can be found in the PDF: **HIS2LIP_supplementary_material.pdf** 
- Model weights and evaluation datasets will be released **after paper acceptance**.  
- Prompts for evaluation are available in **constants.py** file.  
